# -*- coding: utf-8 -*-
"""Coal Quality Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BBuKHmOB7MHgVOn1Wmx5LTcsPKfI1dWM

# Introduction

Coal Quality Inspection â€” End-to-End Vision Pipeline (YOLO Detection + MobileNetV2 Classification + Flask API)
## English
This project reconstructs a full production-grade coal-quality inspection system originally deployed in a mining environment. Because the real project is under NDA, all experiments here are reproduced using publicly available coal imagery datasetsâ€”but the engineering design, modeling steps, and reasoning strictly follow the real pipeline.

The goal is straightforward: **automate the entire process of detecting coal on conveyor belts and estimating its quality in real time**, replacing inconsistent human inspection with a fast, stable, and reproducible computer-vision workflow.

Coal images captured in mines suffer from extreme variability:
- underground images are often very dark  
- conveyor-belt images are sometimes overexposed  
- dust on lenses, vibration, and shadows introduce noise  
- lighting changes constantly across shifts and mining zones  

To address these challenges, the system is designed as a **two-stage vision pipeline**:

### **1. YOLO (v5 + v8) for coal-region detection**  
We first train YOLO models to locate all coal/gangue regions on each conveyor-belt image.  
This step transforms a complex full-image problem into clean ROIs (Regions of Interest), enabling robust downstream classification.  
Both YOLOv5 and YOLOv8 versions are included:
- VOC â†’ YOLO label conversion  
- train/val/test dataset construction  
- lightweight, high-speed YOLOxN models for real-time usage  

### **2. MobileNetV2 for coal-quality classification**  
Once ROIs are extracted, each crop is classified into four coal-quality levels using a MobileNetV2-based model.  
Because quality is mainly a fine-grained texture task, extensive preprocessing and controlled experiments were necessary:
- CLAHE normalization for uneven lighting  
- edge-enhanced variants for ablation  
- backbone comparison under 100-sample stress tests  
- freeze vs. finetune transfer-learning analysis  
- scaling from 100 â†’ 500 â†’ 5000 samples to evaluate robustness  

Across all experiments, **MobileNetV2 (frozen feature extractor)** consistently provided the best balance of accuracy, stability, and deployment latency.

### **3. Flask API for real-time inference**  
Finally, the YOLO detector and MobileNetV2 classifier are connected into a single stateless REST service:
- `POST /predict` accepts an image  
- YOLO detects all coal regions  
- each ROI is classified by MobileNetV2  
- results are aggregated into a structured JSON response  
- outputs include box locations, per-ROI quality, and overall coal-quality distribution  

Model weights are loaded once at startup, keeping inference responsive (<1s) on a small EC2 instance or edge server.

---

## ä¸­æ–‡
æœ¬é¡¹ç›®å®Œæ•´å¤ç°äº†ä¸€ä¸ªåœ¨çœŸå®çŸ¿åœºç¯å¢ƒä¸­éƒ¨ç½²è¿‡çš„**ç…¤è´¨æ™ºèƒ½æ£€æµ‹ç³»ç»Ÿ**ã€‚  
ç”±äºåŸå§‹é¡¹ç›®å— NDA é™åˆ¶ï¼Œè¿™é‡Œä½¿ç”¨å…¬å…±æ•°æ®é›†é‡æ–°è·‘å®Œæ•´æµç¨‹ï¼Œä½†æ‰€æœ‰çš„å»ºæ¨¡é€»è¾‘ã€å·¥ç¨‹è®¾è®¡ã€å¯¹æ¯”å®éªŒå’Œæ¨ç†æ¶æ„éƒ½ä¸çœŸå®ç”Ÿäº§ç‰ˆæœ¬ä¸€è‡´ã€‚

ç³»ç»Ÿçš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š  
**è‡ªåŠ¨åŒ–è¯†åˆ«ä¼ é€å¸¦ä¸Šçš„ç…¤å—ï¼Œå¹¶å®æ—¶åˆ¤æ–­ç…¤è´¨ç­‰çº§ï¼Œæ›¿ä»£äººå·¥æ£€æµ‹çš„ä¸»è§‚æ€§ä¸ä¸ç¨³å®šæ€§ã€‚**

çŸ¿åŒºå›¾åƒå…·æœ‰å¤§é‡å™ªå£°ä¸å…‰ç…§é—®é¢˜ï¼š
- äº•ä¸‹ç¯å¢ƒææš—  
- çš®å¸¦åŒºåŸŸæœ‰æ—¶ä¸¥é‡è¿‡æ›  
- é•œå¤´å¸¸è¢«ç°å°˜è¦†ç›–  
- æœºæ¢°é˜´å½±é¢‘ç¹å‡ºç°  
- å…‰ç…§éšç­æ¬¡å’ŒåŒºåŸŸå‰§çƒˆå˜åŒ–  

ä¸ºè§£å†³ä¸Šè¿°éš¾é¢˜ï¼Œæˆ‘ä»¬å°†ä»»åŠ¡æ‹†æˆä¸€ä¸ª **åŒé˜¶æ®µè§†è§‰ç³»ç»Ÿ**ï¼š

### **1. YOLOï¼ˆv5 + v8ï¼‰è¿›è¡Œç…¤å—åŒºåŸŸæ£€æµ‹**  
é¦–å…ˆè®­ç»ƒ YOLO æ£€æµ‹æ¨¡å‹ï¼Œå®šä½æ¯å¼ å›¾ä¸­çš„ç…¤/çŸ¸çŸ³åŒºåŸŸã€‚  
è¿™ä¸€æ­¥çš„ä½œç”¨æ˜¯æŠŠå¤æ‚çš„æ•´å›¾é—®é¢˜è½¬åŒ–æˆå¹²å‡€çš„ ROI è£å‰ªä»»åŠ¡ï¼Œä¸ºåˆ†ç±»å™¨æä¾›ç¨³å®šè¾“å…¥ã€‚  
æœ¬é¡¹ç›®åŒæ—¶æä¾› YOLOv5 ä¸ YOLOv8 çš„è®­ç»ƒæµç¨‹ï¼š
- VOC â†’ YOLO æ ‡ç­¾è½¬æ¢  
- æ„å»º train/val/test æ ‡å‡†æ•°æ®é›†  
- ä½¿ç”¨ nano/small æ¨¡å‹æ”¯æŒå®æ—¶éƒ¨ç½²  

### **2. MobileNetV2 è¿›è¡Œç…¤è´¨åˆ†ç±»**  
æ£€æµ‹å‡ºçš„ ROI ä¼šè¢«é€å…¥ MobileNetV2 åˆ†ç±»ç½‘ç»œï¼Œåˆ¤æ–­ç…¤å—å±äºå››ç§è´¨é‡ç­‰çº§ã€‚  
ç”±äºç…¤è´¨åˆ¤æ–­é«˜åº¦ä¾èµ–**ç»†ç²’åº¦çº¹ç†ç‰¹å¾**ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å¯¹ç…§å®éªŒï¼š
- CLAHE å…‰ç…§å½’ä¸€åŒ–  
- è¾¹ç¼˜å¢å¼ºçš„ç»“æ„æ¶ˆè  
- 100 å¼ å°æ ·æœ¬ä¸‹çš„ Backbone å‹æµ‹  
- freeze vs finetune çš„è¿ç§»å­¦ä¹ ç­–ç•¥å¯¹æ¯”  
- 100 â†’ 500 â†’ 5000 æ•°æ®è§„æ¨¡ä¸‹çš„å…¨é“¾è·¯æ€§èƒ½è¯„ä¼°  

æœ€ç»ˆï¼Œ**å†»ç»“ç‰¹å¾å±‚çš„ MobileNetV2** åœ¨ç¨³å®šæ€§ã€æ¨ç†é€Ÿåº¦ã€æ¨¡å‹ä½“ç§¯ä¸å‡†ç¡®ç‡ä¹‹é—´è¾¾æˆäº†æœ€ä½³å¹³è¡¡ã€‚

### **3. Flask API å®ç°ç«¯åˆ°ç«¯å®æ—¶æ¨ç†æœåŠ¡**  
æœ€ç»ˆå°† YOLO + MobileNetV2 å°è£…ä¸ºä¸€ä¸ªæ— çŠ¶æ€çš„ REST æœåŠ¡ï¼š
- `POST /predict` ä¼ å…¥å›¾ç‰‡  
- YOLO æ£€æµ‹ç…¤å—ä½ç½®  
- å¯¹æ¯ä¸ª ROI åš MobileNetV2 åˆ†ç±»  
- èšåˆæˆç»“æ„åŒ– JSON è¾“å‡º  
- åŒ…å«æ¯ä¸ªæ¡†çš„é¢„æµ‹ç»“æœä¸æ•´å›¾çš„æ€»ä½“ç…¤è´¨åˆ†å¸ƒ  

æ¨¡å‹åœ¨æœåŠ¡å¯åŠ¨æ—¶ä¸€æ¬¡æ€§åŠ è½½ï¼Œæ¨ç†è¿‡ç¨‹ä¿æŒè½»é‡ï¼Œå»¶è¿Ÿé€šå¸¸åœ¨ 1 ç§’ä»¥å†…ï¼Œéå¸¸é€‚åˆ EC2 æˆ–è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚

# Data Preprocessing

## Lighting Standardization & CLAHE Preprocessing
During data acquisition, we standardized lighting using a 24â€‘strip LED system (8â€¯W per strip) to maintain 1200â€¯Â±â€¯100â€¯Lux, and captured images with a Basler acA4096â€‘40gc industrial camera at 4096Ã—2168 resolution and 42â€¯fps. Despite these controls, future deployment scenarios may encounter uneven lighting. Consequently, to enhance the modelâ€™s robustness and generalization, we apply CLAHE to all collected images, equalizing local contrast and mitigating illumination variation.

Parameter Selection for CLAHE


*   clipLimit controls the maximum amount of contrast enhancement in each local region. The default value in OpenCV is 40, but this is often too aggressive and may lead to inconsistent detail enhancement. A value between 2.0 and 5.0 is typically preferred, as it improves local contrast while avoiding excessive sharpening and noise amplification.
*   tileGridSize defines how the image is divided into smaller tiles. Each tile is processed individually using histogram equalization, and the results are blended with bilinear interpolation to ensure smooth transitions. A grid size of (8, 8) is the default and provides a good balance: it enhances local details without producing visible block artifacts




.

# 1. Coal Image Classification Pipeline â€” Section Overview  
## English  

This section documents the complete coal-quality classification pipeline that we originally engineered and deployed in a real mining production environment. Due to NDA constraints, the version presented here is a fully aligned reconstruction using a large public dataset (e.g., DsCGF ~270k raw coal images). All preprocessing steps, model comparison logic, and engineering decisions strictly follow the real production pipeline, ensuring that the relative outcomes remain valid even though the absolute performance metrics differ.

Coal quality inspection in mining operations is traditionally manual and highly subjective. Different inspectors may apply different quality standards, and lighting varies dramatically across tunnels and conveyor belts. These inconsistencies motivated us to build a fully automated, reproducible, and explainable classification system that could operate reliably under real-world constraints.

We formulate the task as a **4-class supervised learning problem**:

- **High-quality coal** â€” >80% coal surface area  
- **Acceptable-quality coal** â€” 60â€“80%  
- **Poor-quality coal** â€” <60%  
- **Background / non-coal regions**

From an engineering perspective, the pipeline must solve four challenges simultaneously:

1. **Lighting variability**  
   Coal images may be too dark (underground) or overexposed (belt surface). CNNs trained naÃ¯vely on such data tend to overfit lighting instead of material texture. This motivates CLAHE and other normalization strategies.

2. **Texture-driven classification**  
   Coal quality is fundamentally a texture problemâ€”fine-grained, subtle differences in surface roughness and fragment density. The model must capture these patterns robustly, even under uneven illumination.

3. **Data scarcity vs model capacity**  
   Real mining environments rarely offer perfectly labeled thousands-of-samples datasets at the beginning. This requires careful evaluation of freeze vs finetune strategies, especially under <500-image conditions.

4. **Deployment constraints**  
   The final model must run in real-time in a resource-constrained environment (edge servers / low-power EC2 instances). This imposes strict latency and memory constraints, naturally pushing us toward lightweight architectures like MobileNetV2.

Across the next subsections, we progressively move through the full engineering pipeline:

- **Data Normalization (CLAHE, Edge Extraction)**  
- **Backbone Benchmarking on Small-scale Data (100 samples)**  
- **Transfer-learning Strategies under Scarcity (Freeze vs Finetune)**  
- **Scaling Up Experiments (500 â†’ 5000 samples)**  
- **Final Model Selection for Production (MobileNetV2_freeze)**  

This structure mirrors how real teams make decisions: start with robustness, test hypotheses under low-resource conditions, verify scaling behavior, then pick a model that balances accuracy, latency, stability, and deployment cost.

---

## ä¸­æ–‡  

æœ¬èŠ‚è®°å½•çš„æ˜¯æˆ‘ä»¬åœ¨çœŸå®çŸ¿åœºç”Ÿäº§ç¯å¢ƒä¸­è®¾è®¡å¹¶éƒ¨ç½²è¿‡çš„ **ç…¤è´¨æ™ºèƒ½è¯†åˆ«ç®¡çº¿ï¼ˆCoal Quality Classification Pipelineï¼‰**ã€‚  
ç”±äºå—åˆ° NDA é™åˆ¶ï¼Œè¿™é‡Œå±•ç¤ºçš„ç‰ˆæœ¬åŸºäºå…¬å¼€æ•°æ®é›†ï¼ˆå¦‚ DsCGFï¼Œçº¦ 27 ä¸‡å¼ ç…¤ç‚­å›¾åƒï¼‰è¿›è¡Œäº†å®Œæ•´å¤ç°ï¼Œä½†æ‰€æœ‰çš„ **é¢„å¤„ç†æ­¥éª¤ã€æ¨¡å‹æ¯”è¾ƒé€»è¾‘ã€å·¥ç¨‹åŒ–å†³ç­–** éƒ½ä¸çœŸå®ç³»ç»Ÿä¿æŒä¸€è‡´ï¼Œä»è€Œç¡®ä¿â€œç›¸å¯¹ç»“è®ºâ€å®Œå…¨æœ‰æ•ˆã€‚

ä¼ ç»Ÿçš„ç…¤è´¨æ£€éªŒä¾èµ–äººå·¥ï¼Œä¸»è§‚æ€§æå¼ºï¼Œæ£€éªŒå‘˜ä¹‹é—´å®¹æ˜“å‡ºç°æ ‡å‡†ä¸ä¸€è‡´çš„é—®é¢˜ï¼›åŒæ—¶çŸ¿åŒºå…‰ç…§æä¸ç¨³å®šï¼Œäº•ä¸‹ç¯å¢ƒæ˜æš—ã€ä¼ é€å¸¦åŒºåŸŸåˆå¯èƒ½è¿‡æ›ã€‚æ‰€æœ‰è¿™äº›å› ç´ éƒ½é©±åŠ¨æˆ‘ä»¬æ„å»ºä¸€ä¸ª **æ ‡å‡†åŒ–ã€å¯å¤ç°ã€å¯è§£é‡Šã€å¯å¤§è§„æ¨¡éƒ¨ç½²çš„æ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ**ã€‚

åœ¨å¤ç°è®¾è®¡ä¸­ï¼Œæˆ‘ä»¬å°†ä»»åŠ¡å®šä¹‰ä¸ºä¸€ä¸ª **å››åˆ†ç±»ç›‘ç£å­¦ä¹ é—®é¢˜**ï¼š

- **é«˜å“è´¨ç…¤** â€” è¡¨é¢ç§¯ > 80%  
- **ä¸­ç­‰å“è´¨ç…¤** â€” 60â€“80%  
- **ä½å“è´¨ç…¤** â€” < 60%  
- **èƒŒæ™¯ / éç…¤åŒºåŸŸ**

ä»å·¥ç¨‹è§’åº¦çœ‹ï¼Œè¿™å¥—ç³»ç»Ÿéœ€è¦åŒæ—¶è§£å†³å››ä¸ªæ ¸å¿ƒéš¾é¢˜ï¼š

1. **å…‰ç…§å˜åŒ–å·¨å¤§**  
   ç…¤çŸ¿å›¾åƒææ˜“è¿‡æš—æˆ–è¿‡äº®ï¼Œæœªç»å¤„ç†ç›´æ¥è®­ç»ƒ CNN ä¼šå¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°â€œå…‰ç…§â€ï¼Œè€Œéâ€œæè´¨çº¹ç†â€ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬é‡‡ç”¨ CLAHE ç­‰å½’ä¸€åŒ–æ–¹æ³•ã€‚

2. **åŸºäºçº¹ç†çš„ç»†ç²’åº¦åˆ†ç±»**  
   ç…¤è´¨æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªçº¹ç†é—®é¢˜â€”â€”å·®å¼‚å¾®å°ä½†å…³é”®ï¼Œéœ€è¦æ¨¡å‹åœ¨å™ªå£°ã€å…‰ç…§å˜åŒ–ä¸‹ä»èƒ½ä¿æŒç¨³å¥ã€‚

3. **æ•°æ®ç¨€ç¼ºä¸æ¨¡å‹å®¹é‡çš„å¹³è¡¡**  
   çŸ¿åœºå¼€å§‹éƒ¨ç½²æ—¶ï¼Œé€šå¸¸ä¸å¯èƒ½ç«‹åˆ»æ‹¿åˆ°æˆåƒä¸Šä¸‡å¼ å¸¦ç²¾ç¡®æ ‡æ³¨çš„æ•°æ®ã€‚å› æ­¤æˆ‘ä»¬å¿…é¡»è°¨æ…æ¯”è¾ƒ freeze ä¸ finetune ç­–ç•¥ï¼Œå°¤å…¶æ˜¯åœ¨ 100â€“500 æ ·æœ¬è§„æ¨¡ä¸‹ã€‚

4. **éƒ¨ç½²ä¾§çº¦æŸï¼ˆå»¶è¿Ÿ / å†…å­˜ / å¹¶å‘ï¼‰**  
   æœ€ç»ˆæ¨¡å‹éœ€è¦åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œï¼ˆè¾¹ç¼˜æœåŠ¡å™¨ã€æˆ–ä½é… EC2 å®ä¾‹ï¼‰ã€‚è¿™ä½¿å¾—è½»é‡æ¨¡å‹ï¼ˆå¦‚ MobileNetV2ï¼‰æˆä¸ºæ›´åˆç†çš„é€‰æ‹©ã€‚

æ¥ä¸‹æ¥çš„å°èŠ‚å°†é€æ­¥å±•ç¤ºæ•´ä¸ªå·¥ç¨‹æµæ°´çº¿ï¼š

- **æ•°æ®å½’ä¸€åŒ–ï¼ˆCLAHEã€è¾¹ç¼˜æå–ï¼‰**  
- **å°æ ·æœ¬æ¡ä»¶ä¸‹çš„ Backbone Benchmarkï¼ˆ100 å¼ ï¼‰**  
- **è¿ç§»å­¦ä¹ ç­–ç•¥å¯¹æ¯”ï¼ˆFreeze vs Finetuneï¼‰**  
- **æ‰©å¤§æ•°æ®è§„æ¨¡åçš„è¡¨ç°ï¼ˆ500 â†’ 5000ï¼‰**  
- **æœ€ç»ˆæ¨¡å‹é€‰æ‹©ï¼ˆMobileNetV2_freeze ä½œä¸ºç”Ÿäº§æ¨¡å‹ï¼‰**

è¿™å¥—ç»“æ„ä¸çœŸå®å·¥ç¨‹å›¢é˜Ÿçš„å†³ç­–é€»è¾‘å®Œå…¨ä¸€è‡´ï¼š  
å…ˆä¿è¯é²æ£’æ€§ â†’ å†åœ¨ä½èµ„æºæ¡ä»¶ä¸‹æ£€éªŒå‡è®¾ â†’ å†éªŒè¯æ‰©å±•èƒ½åŠ› â†’ æœ€ç»ˆé€‰æ‹©â€œç²¾åº¦ + é€Ÿåº¦ + ç¨³å®šæ€§ + æˆæœ¬â€æœ€ä¼˜è§£ã€‚

## 1.1 Preprocessing Pipelineï¼ˆCLAHE + Edgeï¼‰
## English
This part of the pipeline corresponds to two Colab cells:  
one generates the CLAHE-normalized dataset, and the other optionally extracts edges from those CLAHE images.  
They work together because both steps serve the same purpose: **make the dataset more stable before we test any backbone**.

The CLAHE cell normalizes uneven lightingâ€”something that happens constantly in mining environments. By enhancing only the L-channel in LAB space, the images maintain consistent texture contrast without artificial color distortion.

The second cell builds an edge-based variant. Itâ€™s not meant to improve accuracy directly but to give us a different â€œstructural perspectiveâ€ of the same data. Running the same models on both versions helps us understand which backbones rely heavily on fine texture vs higher-level structure.

Together, these two preprocessing steps give us controlled input conditions, making later benchmarking interpretable instead of noisy guesswork.

## ä¸­æ–‡
è¿™ä¸€éƒ¨åˆ†å¯¹åº”ä¸¤ä¸ª Colab cellï¼š  
ä¸€ä¸ªç”¨äºç”Ÿæˆ CLAHE å…‰ç…§å½’ä¸€åŒ–æ•°æ®é›†ï¼Œå¦ä¸€ä¸ªåœ¨ CLAHE åŸºç¡€ä¸Šæå–è¾¹ç¼˜ã€‚  
æŠŠè¿™ä¸¤ä¸ªæ­¥éª¤æ”¾åœ¨ä¸€èµ·ï¼Œæ˜¯å› ä¸ºå®ƒä»¬éƒ½å±äº **æ•°æ®ç¨³å®šåŒ–çš„é¢„å¤„ç†æµç¨‹**ï¼Œåœ¨ä»»ä½•æ¨¡å‹è®­ç»ƒä¹‹å‰å¿…é¡»å…ˆåšå¥½ã€‚

CLAHE ç”¨äºè§£å†³çŸ¿åŒºå›¾åƒæœ€å¸¸è§çš„å…‰ç…§ä¸å‡é—®é¢˜ã€‚é€šè¿‡å¢å¼º LAB ç©ºé—´ä¸­çš„äº®åº¦é€šé“ï¼Œçº¹ç†ä¿¡æ¯æ›´æ¸…æ™°ï¼ŒåŒæ—¶ä¸ä¼šå¼•å…¥é¢œè‰²åå·®ã€‚

ç¬¬äºŒä¸ª cell ä¼šåœ¨ CLAHE å›¾åƒä¸Šæå–è¾¹ç¼˜ã€‚å®ƒçš„ç›®çš„ä¸æ˜¯æå‡å‡†ç¡®ç‡ï¼Œè€Œæ˜¯æä¾›ä¸€ç§â€œç»“æ„è§†è§’â€ã€‚åŒæ ·çš„æ¨¡å‹åœ¨ä¸¤å¥—æ•°æ®ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œèƒ½å¸®åŠ©æˆ‘ä»¬åˆ¤æ–­æ¨¡å‹åˆ°åº•ä¾èµ–çº¹ç†è¿˜æ˜¯è½®å»“ç»“æ„ã€‚

æœ‰äº†è¿™ä¸¤å¥—é¢„å¤„ç†æ•°æ®ï¼Œæˆ‘ä»¬åç»­çš„æ¨¡å‹å¯¹æ¯”å°±æ›´å¯é ï¼Œä¸ä¼šå› ä¸ºå…‰ç…§æˆ–å™ªå£°å¹²æ‰°å¯¼è‡´é”™è¯¯ç»“è®ºã€‚
"""

# ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥åº“ & æŒ‚è½½ Drive
import os, cv2
from tqdm import tqdm
from google.colab import drive
drive.mount('/content/drive')

# CLAHE å¢å¼ºå‡½æ•°
def apply_clahe(img_path, clip_limit=2.0, grid_size=(8,8)):
    img = cv2.imread(img_path)
    if img is None:
        return None
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)
    cl = clahe.apply(l)
    limg = cv2.merge((cl, a, b))
    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)
    return final

# è¦å¤„ç†çš„å­é›†åˆ—è¡¨
base_src = "/content/drive/MyDrive/coal/image_classification"
base_dst = "/content/drive/MyDrive/coal/image_classification_Clahe"  # å¯æ ¹æ®éœ€è¦æ›´æ”¹è¾“å‡ºç›®å½•

subsets = ["train_100", "train_500", "val", "test"]

for subset in subsets:
    src_dir = os.path.join(base_src, subset)
    dst_dir = os.path.join(base_dst, subset + "_Clahe")
    os.makedirs(dst_dir, exist_ok=True)
    print(f"\nProcessing subset: {subset} â†’ {dst_dir}")
    for class_name in os.listdir(src_dir):
        src_class = os.path.join(src_dir, class_name)
        dst_class = os.path.join(dst_dir, class_name)
        os.makedirs(dst_class, exist_ok=True)
        for fname in tqdm(os.listdir(src_class), desc=f"{subset}/{class_name}", leave=False):
            src_path = os.path.join(src_class, fname)
            dst_path = os.path.join(dst_class, fname)
            try:
                enhanced = apply_clahe(src_path, clip_limit=2.0, grid_size=(8,8))
                if enhanced is not None:
                    cv2.imwrite(dst_path, enhanced)
            except Exception as e:
                # å¯æ ¹æ®éœ€è¦æ‰“å° e
                continue

print("\nâœ… All subsets processed and CLAHE images saved.")

import os, cv2
from tqdm import tqdm
from google.colab import drive
drive.mount('/content/drive')

def apply_canny_on_clahe(img_path, t_lower=50, t_upper=150):
    img = cv2.imread(img_path)
    if img is None: return None
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (5, 5), 1.4)
    edges = cv2.Canny(blurred, threshold1=t_lower, threshold2=t_upper)
    return cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)

base_clahe = "/content/drive/MyDrive/coal/image_classification_Clahe"
base_edge = "/content/drive/MyDrive/coal/image_classification_Clahe_Edge"
subsets = ["train_100", "train_500", "val", "test"]

for subset in subsets:
    src_dir = os.path.join(base_clahe, subset + "_Clahe")
    dst_dir = os.path.join(base_edge, subset + "_Clahe_Edge")
    os.makedirs(dst_dir, exist_ok=True)
    print(f"Processing {subset} => edge maps saved to {dst_dir}")
    for cls in os.listdir(src_dir):
        src_cls = os.path.join(src_dir, cls)
        dst_cls = os.path.join(dst_dir, cls)
        os.makedirs(dst_cls, exist_ok=True)
        for fname in tqdm(os.listdir(src_cls), desc=f"{subset}/{cls}", leave=False):
            src_path = os.path.join(src_cls, fname)
            dst_path = os.path.join(dst_cls, fname)
            try:
                img_edge = apply_canny_on_clahe(src_path)
                if img_edge is not None:
                    cv2.imwrite(dst_path, img_edge)
            except:
                continue

print("âœ… All edge-enhanced datasets generated!")

"""## 1.2 Small-Scale Backbone Benchmarkï¼ˆ100-sampleï¼‰
## English
This block corresponds to the Colab cell where we benchmark five backbones (MobileNetV2, ResNet50, EfficientNetB0/B3, DenseNet121) using only 100 training images.  
The idea is simple: **stress-test each backbone under extremely limited data**.

With only 100 samples, unstable models will overfit immediately, sensitive models will diverge, and robust models will show smooth validation curves.  
This is a quick way to eliminate architectures before wasting compute on larger experiments.

In practice, MobileNetV2 behaves the most predictably, DenseNet121 often spikes then collapses, and EfficientNet variants show inconsistent curves.  
Edge-enhanced data tends to destabilize most models, which also provides useful signal about backbone sensitivity.

This small test doesnâ€™t aim for high accuracyâ€”it aims to reveal model behavior. That behavior determines which models deserve further evaluation.

## ä¸­æ–‡
è¿™ä¸€éƒ¨åˆ†å¯¹åº” Colab ä¸­å¯¹äº”ç§ Backboneï¼ˆMobileNetV2ã€ResNet50ã€EfficientNetB0/B3ã€DenseNet121ï¼‰çš„ 100 å¼ å°æ ·æœ¬åŸºå‡†æµ‹è¯•ã€‚  
æ ¸å¿ƒç›®çš„å¾ˆç›´æ¥ï¼š**ç”¨æå°çš„æ•°æ®é‡å»å‹æµ‹æ¨¡å‹çš„ç¨³å®šæ€§**ã€‚

åœ¨ 100 å¼ æ•°æ®ä¸Šï¼Œä¸ç¨³å®šçš„æ¨¡å‹ä¼šç«‹åˆ»å´©æ‰ã€è¿‡æ‹Ÿåˆæˆ–å‡ºç°å·¨å¹…æ³¢åŠ¨ï¼›  
è€Œç¨³å®šçš„æ¨¡å‹ä¼šç»™å‡ºå¹³æ»‘ã€å¯é¢„æœŸçš„éªŒè¯æ›²çº¿ã€‚

å®é™…ç»“æœä¸­ï¼ŒMobileNetV2 è¡¨ç°æœ€ç¨³å®šï¼›DenseNet121 è™½ç„¶æœ‰é«˜å³°ä½†éå¸¸ä¸å¯æ§ï¼›EfficientNet ç³»åˆ—åœ¨å°æ•°æ®ä¸‹è¡¨ç°ä¸ä¸€è‡´ï¼›  
è€Œ Edge æ•°æ®ç‰ˆæœ¬ä¼šè®©å¤§éƒ¨åˆ†æ¨¡å‹æ›´åŠ ä¸ç¨³ï¼Œè¿™ä¹Ÿè¯´æ˜æ¨¡å‹å¯¹ç¨€ç–ç»“æ„ä¿¡å·å¾ˆæ•æ„Ÿã€‚

è¿™ä¸€æ­¥å¹¶ä¸æ˜¯ä¸ºäº†è¿½æ±‚å‡†ç¡®ç‡ï¼Œè€Œæ˜¯ä¸ºäº†çœ‹æ¨¡å‹â€œæ€§æ ¼â€ã€‚  
ç¨³å®šã€æœ‰è§„å¾‹çš„æ¨¡å‹ï¼Œæ‰å€¼å¾—è¿›å…¥ä¸‹ä¸€é˜¶æ®µã€‚


"""

# âœ… Colab å¯è¿è¡Œï¼šCLAHE ä¸ CLAHE+Edge è¾“å…¥çš„ 5 æ¨¡å‹è®­ç»ƒä¸éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”
# æ”¯æŒ MobileNetV2, ResNet18, EfficientNetB0/B3, ShuffleNetV2, DenseNet121

# âœ… ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥åº“å’ŒæŒ‚è½½ Drive
import os
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from google.colab import drive
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

drive.mount('/content/drive')

# âœ… ç¬¬äºŒæ­¥ï¼šæ•°æ®è·¯å¾„
clahe_dir = "/content/drive/MyDrive/coal/image_classification_Clahe/train_100_Clahe"
clahe_edge_dir = "/content/drive/MyDrive/coal/image_classification_Clahe_Edge/train_100_Clahe_Edge"
val_dir = "/content/drive/MyDrive/coal/image_classification/val"

# âœ… ç¬¬ä¸‰æ­¥ï¼šæ•°æ®ç”Ÿæˆå™¨
img_size = (224, 224)
batch_size = 32

data_gen = ImageDataGenerator(rescale=1./255)

def get_generators(input_dir):
    train_gen = data_gen.flow_from_directory(
        input_dir,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical'
    )
    val_gen = data_gen.flow_from_directory(
        val_dir,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical'
    )
    return train_gen, val_gen

# âœ… ç¬¬å››æ­¥ï¼šæ¨¡å‹æ„å»ºå‡½æ•°
from tensorflow.keras.applications import MobileNetV2, EfficientNetB0, EfficientNetB3, DenseNet121, ResNet50

def build_model(base_model):
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    output = Dense(4, activation='softmax')(x)
    return Model(inputs=base_model.input, outputs=output)

# âœ… ç¬¬äº”æ­¥ï¼šæ¨¡å‹åˆ—è¡¨ä¸è®­ç»ƒ
model_defs = {
    'MobileNetV2': tf.keras.applications.MobileNetV2,
    'ResNet50': tf.keras.applications.ResNet50,
    'EfficientNetB0': tf.keras.applications.EfficientNetB0,
    'EfficientNetB3': tf.keras.applications.EfficientNetB3,
    'DenseNet121': tf.keras.applications.DenseNet121,
}

history_dict = {}
epochs = 20

for model_name, model_fn in model_defs.items():
    for input_type, input_dir in zip(['CLAHE', 'CLAHE_EDGE'], [clahe_dir, clahe_edge_dir]):
        print(f"\nğŸ”§ Training {model_name} on {input_type}...")
        base = model_fn(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
        for layer in base.layers[:-3]:
            layer.trainable = False
        model = build_model(base)
        model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

        train_gen, val_gen = get_generators(input_dir)
        history = model.fit(
            train_gen,
            validation_data=val_gen,
            epochs=epochs,
            callbacks=[EarlyStopping(patience=3, restore_best_weights=True)],
            verbose=1
        )
        history_dict[f"{model_name}_{input_type}"] = history.history['val_accuracy']

# âœ… ç¬¬å…­æ­¥ï¼šç»˜å›¾æ¯”è¾ƒ
plt.figure(figsize=(14,6))
for name, acc in history_dict.items():
    plt.plot(acc, label=name)
plt.title("Validation Accuracy Comparison")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

"""## 1.3 Transfer Learning Strategyï¼ˆFreeze vs Finetuneï¼‰
## English
This block corresponds to the Colab cell comparing two transfer learning strategies:  
freezing all backbone layers vs fine-tuning only the last few layers.

The goal of this experiment is to understand how each backbone behaves under small-to-medium data sizes.  
When the dataset is limited, fine-tuning tends to amplify noise and destabilize early layers, while freezing keeps the feature extractor consistent.

The results reflect exactly that:  
MobileNetV2_freeze is the most stable across datasets,  
DenseNet121_finetune achieves higher peaks but fluctuates heavily,  
and ResNet50 behaves similarly in both modes.

These findings help us converge on a practical rule:  
**for datasets below ~2000 images, freezing is generally the safer and more consistent choice.**

## ä¸­æ–‡
è¿™ä¸€éƒ¨åˆ†å¯¹åº” Colab ä¸­ freeze ä¸ finetune ä¸¤ç§ç­–ç•¥çš„å¯¹æ¯”å®éªŒã€‚  
ç›®æ ‡æ˜¯è¯„ä¼°ä¸åŒ Backbone åœ¨å°æ ·æœ¬åˆ°ä¸­æ ·æœ¬è§„æ¨¡ä¸‹çš„å¯é æ€§ã€‚

åœ¨æ•°æ®ä¸å¤šçš„æ—¶å€™ï¼Œfinetune å¾€å¾€ä¼šæ”¾å¤§å™ªå£°ï¼Œä½¿å‰å‡ ä¸ªå·ç§¯å±‚å˜å¾—ä¸ç¨³å®šï¼›  
ç›¸æ¯”ä¹‹ä¸‹ï¼Œfreeze ä¼šè®© Backbone å½“ä½œä¸€ä¸ªç¨³å®šçš„ç‰¹å¾æå–å™¨ï¼Œæ›´å®¹æ˜“æ”¶æ•›ã€‚

å®éªŒç»“æœéå¸¸ç¬¦åˆé¢„æœŸï¼š  
MobileNetV2_freeze åœ¨å„ç§è®¾ç½®ä¸‹éƒ½æœ€ç¨³å®šï¼›  
DenseNet121_finetune è™½ç„¶å³°å€¼é«˜ä½†æ³¢åŠ¨éå¸¸å¤§ï¼›  
ResNet50 ä¸¤ç§æ¨¡å¼è¡¨ç°æ¥è¿‘ã€‚

ç”±æ­¤å¯ä»¥æ€»ç»“å‡ºä¸€ä¸ªç»éªŒè§„å¾‹ï¼š  
**å½“æ•°æ®é‡ä¸è¶³æ—¶ï¼Œfreeze æ˜¯æ›´ç¨³ã€æ›´å¯æ§çš„é€‰æ‹©ã€‚**


"""

# âœ… Colab ä»£ç ï¼šMobileNetV2ã€ResNet50(æ›¿ä»£18)ã€DenseNet121
# è®­ç»ƒåŸå§‹(image_classification)ä¸CLAHE(image_classification_Clahe)ä¸¤å¥—æ•°æ®
# å¯¹æ¯”ã€Œä¸å¾®è°ƒã€ä¸ã€Œå¾®è°ƒæœ€åä¸‰å±‚ã€ä¸¤ç§ç­–ç•¥

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# âœ… å‚æ•°è®¾ç½®
img_size = 224
batch_size = 16
epochs = 20

# âœ… æ•°æ®è·¯å¾„
datasets = {
    "Original": "/content/drive/MyDrive/coal/image_classification/train_100",
    "CLAHE": "/content/drive/MyDrive/coal/image_classification_Clahe/train_100_Clahe"
}

# âœ… æ¨¡å‹æ„å»ºå‡½æ•°
def build_model(base_model, num_classes):
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    out = Dense(num_classes, activation='softmax')(x)
    return Model(inputs=base_model.input, outputs=out)

# âœ… æ¨¡å‹å®šä¹‰
model_defs = {
    "MobileNetV2": tf.keras.applications.MobileNetV2,
    "ResNet50": tf.keras.applications.ResNet50,
    "DenseNet121": tf.keras.applications.DenseNet121
}

# âœ… EarlyStopping è®¾ç½®
def get_early_stop():
    return EarlyStopping(monitor='val_accuracy', mode='max', patience=3,
                         min_delta=0.002, restore_best_weights=True, verbose=1)

# âœ… è®­ç»ƒå‡½æ•°
def train_model(base, num_classes, trainable_mode):
    if trainable_mode == "freeze":
        for layer in base.layers:
            layer.trainable = False
    elif trainable_mode == "finetune_last3":
        for layer in base.layers[:-3]:
            layer.trainable = False
    model = build_model(base, num_classes)
    model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# âœ… ä¸»å¾ªç¯ï¼šéå†æ•°æ®é›† + æ¨¡å‹ + è®­ç»ƒç­–ç•¥
for ds_name, ds_path in datasets.items():
    print(f"\nğŸ“‚ Training on Dataset: {ds_name}")
    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

    train_gen = datagen.flow_from_directory(
        ds_path, target_size=(img_size, img_size), batch_size=batch_size,
        class_mode='categorical', subset='training', shuffle=True)
    val_gen = datagen.flow_from_directory(
        ds_path, target_size=(img_size, img_size), batch_size=batch_size,
        class_mode='categorical', subset='validation', shuffle=False)

    histories = {}
    for model_name, constructor in model_defs.items():
        for mode in ["freeze", "finetune_last3"]:
            label = f"{model_name}_{mode}"
            print(f"\nğŸ”§ Training {label} on {ds_name}...")
            base_model = constructor(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))
            model = train_model(base_model, train_gen.num_classes, mode)
            hist = model.fit(train_gen, validation_data=val_gen, epochs=epochs,
                             callbacks=[get_early_stop()], verbose=2)
            histories[label] = hist.history['val_accuracy']

    # âœ… ç»˜å›¾ï¼šæ¯ä¸ªæ•°æ®é›†å•ç‹¬ä¸€å¼ å›¾
    plt.figure(figsize=(10,6))
    for name, acc in histories.items():
        plt.plot(acc, label=name)
    plt.title(f"{ds_name} Validation Accuracy (Freeze vs Fine-tune Last 3)")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid(True)
    plt.show()

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# âœ… å‚æ•°
img_size = 224
batch_size = 16
epochs = 20

# âœ… æ•°æ®è·¯å¾„
datasets = {
    "Original": {
        "train": "/content/drive/MyDrive/coal/image_classification/train_500",
        "val": "/content/drive/MyDrive/coal/image_classification/val",
        "test": "/content/drive/MyDrive/coal/image_classification/test",
    },
    "CLAHE": {
        "train": "/content/drive/MyDrive/coal/image_classification_Clahe/train_500_Clahe",
        "val": "/content/drive/MyDrive/coal/image_classification_Clahe/val_Clahe",
        "test": "/content/drive/MyDrive/coal/image_classification_Clahe/test_Clahe",
    }
}

# âœ… æ¨¡å‹æ„å»ºå‡½æ•°
def build_model(base_model, num_classes):
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    out = Dense(num_classes, activation='softmax')(x)
    return Model(inputs=base_model.input, outputs=out)

# âœ… EarlyStopping
def get_early_stop():
    return EarlyStopping(monitor='val_accuracy', patience=3, min_delta=0.002,
                         restore_best_weights=True, verbose=1)

# âœ… è®­ç»ƒå‡½æ•°
def train_model(base, num_classes, trainable_mode):
    if trainable_mode == "freeze":
        for layer in base.layers:
            layer.trainable = False
    elif trainable_mode == "finetune_last3":
        for layer in base.layers[:-3]:
            layer.trainable = False
    model = build_model(base, num_classes)
    model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# âœ… è®­ç»ƒ & æµ‹è¯•å‡½æ•°
def run_training(ds_name, ds_paths):
    datagen = ImageDataGenerator(rescale=1./255)
    train_gen = datagen.flow_from_directory(ds_paths["train"], target_size=(img_size, img_size),
                                            batch_size=batch_size, class_mode='categorical')
    val_gen = datagen.flow_from_directory(ds_paths["val"], target_size=(img_size, img_size),
                                          batch_size=batch_size, class_mode='categorical', shuffle=False)
    test_gen = datagen.flow_from_directory(ds_paths["test"], target_size=(img_size, img_size),
                                           batch_size=batch_size, class_mode='categorical', shuffle=False)

    # âœ… éœ€è¦è®­ç»ƒçš„æ¨¡å‹
    configs = [
        ("MobileNetV2_freeze", tf.keras.applications.MobileNetV2, "freeze"),
        ("DenseNet121_freeze", tf.keras.applications.DenseNet121, "freeze"),
        ("DenseNet121_finetune_last3", tf.keras.applications.DenseNet121, "finetune_last3"),
    ]

    histories, test_accs = {}, {}

    for label, constructor, mode in configs:
        print(f"\nğŸ”§ Training {label} on {ds_name}...")
        base_model = constructor(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))
        model = train_model(base_model, train_gen.num_classes, mode)
        hist = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=[get_early_stop()], verbose=2)
        histories[label] = hist.history['val_accuracy']

        # âœ… æµ‹è¯•å‡†ç¡®ç‡
        loss, acc = model.evaluate(test_gen, verbose=0)
        test_accs[label] = acc

    # âœ… ç»˜åˆ¶ val_accuracy
    plt.figure(figsize=(10,6))
    for name, acc in histories.items():
        plt.plot(acc, label=name)
    plt.title(f"{ds_name} Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid(True)
    plt.show()

    # âœ… ç»˜åˆ¶ test accuracy æ¡å½¢å›¾
    plt.figure(figsize=(8,5))
    plt.bar(test_accs.keys(), test_accs.values())
    plt.title(f"{ds_name} Test Accuracy")
    plt.ylabel("Accuracy")
    plt.xticks(rotation=15)
    plt.show()

# âœ… ä¸»ç¨‹åºï¼šåŸå§‹ & CLAHE
for ds, paths in datasets.items():
    print(f"\nğŸ“‚ Running experiments on: {ds}")
    run_training(ds, paths)

"""## 1.4 Scaling Upï¼ˆ500 â†’ 5000ï¼‰& Final Model Choice
## English
This block summarizes the Colab cells where the dataset is expanded from 500 to 5000 images.  
The purpose is not only to improve accuracy but to observe how each candidate model scales as data increases.

With 500 images, we start to see clearer differences:  
MobileNetV2_freeze stays stable and efficient,  
DenseNet121_finetune reaches high accuracy but is too heavy for real-time use,  
and CLAHE's benefit becomes smaller as lighting variance is reduced by data volume.

The 5000-sample experiment confirms the trend.  
MobileNetV2_freeze reaches solid validation and test accuracy while maintaining low latency and small model size.  
This makes it the most suitable model for downstream integration with YOLO detection and Flask deployment.

The final exported model is:  
`mobilenetv2_freeze_model.h5`

## ä¸­æ–‡
è¿™ä¸€éƒ¨åˆ†å¯¹åº” Colab ä¸­å°†è§„æ¨¡ä» 500 æ‰©å±•åˆ° 5000 çš„è®­ç»ƒå®éªŒã€‚  
è¿™ä¸€æ­¥çš„é‡ç‚¹ä¸åªæ˜¯æå‡å‡†ç¡®ç‡ï¼Œæ›´é‡è¦çš„æ˜¯è§‚å¯Ÿæ¨¡å‹éšæ•°æ®å¢é•¿çš„ç¨³å®šæ€§å’Œæ‰©å±•èƒ½åŠ›ã€‚

åœ¨ 500 æ•°æ®è§„æ¨¡ä¸‹ï¼Œå·®å¼‚å˜å¾—æ˜æ˜¾ï¼š  
MobileNetV2_freeze ç¨³å®šæ€§æœ€å¥½ï¼›  
DenseNet121_finetune è™½ç„¶ç²¾åº¦é«˜ä½†å¤ªé‡ï¼Œä¸é€‚åˆå®æ—¶éƒ¨ç½²ï¼›  
éšç€æ•°æ®å¢å¤šï¼ŒCLAHE çš„ä¼˜åŠ¿ç›¸å¯¹å˜å¼±ï¼Œå› ä¸ºå…‰ç…§å·®å¼‚å¼€å§‹è¢«æ•°æ®é‡å¹³å‡æ‰ã€‚

åœ¨ 5000 æ•°æ®è§„æ¨¡ä¸‹ï¼Œè¶‹åŠ¿å®Œå…¨æ˜ç¡®ï¼š  
MobileNetV2_freeze å‡†ç¡®ç‡ç¨³å¥ã€æ¨¡å‹è½»é‡ã€æ¨ç†é€Ÿåº¦å¿«ï¼Œéå¸¸é€‚åˆåç»­ä¸ YOLO ç»“åˆå¹¶ä¸Šçº¿åˆ° Flask æœåŠ¡ã€‚

æœ€ç»ˆè¾“å‡ºçš„æ¨¡å‹ä¸ºï¼š  
`mobilenetv2_freeze_model.h5`


"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# âœ… å‚æ•°è®¾ç½®
img_size = 224
batch_size = 16
epochs = 20

# âœ… æ•°æ®è·¯å¾„ï¼ˆæ›´å¤§çš„è®­ç»ƒé›†ï¼‰
ds_name = "MobileNetV2_5000"
ds_paths = {
    "train": "/content/drive/MyDrive/coal/image_classification/train_5000",
    "val": "/content/drive/MyDrive/coal/image_classification/val",
    "test": "/content/drive/MyDrive/coal/image_classification/test"
}

# âœ… æ•°æ®ç”Ÿæˆå™¨
datagen = ImageDataGenerator(rescale=1./255)
train_gen = datagen.flow_from_directory(ds_paths["train"], target_size=(img_size, img_size),
                                        batch_size=batch_size, class_mode='categorical')
val_gen = datagen.flow_from_directory(ds_paths["val"], target_size=(img_size, img_size),
                                      batch_size=batch_size, class_mode='categorical', shuffle=False)
test_gen = datagen.flow_from_directory(ds_paths["test"], target_size=(img_size, img_size),
                                       batch_size=batch_size, class_mode='categorical', shuffle=False)

# âœ… æ„å»ºæ¨¡å‹
def build_model(base_model, num_classes):
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    out = Dense(num_classes, activation='softmax')(x)
    return Model(inputs=base_model.input, outputs=out)

# âœ… EarlyStopping
early_stop = EarlyStopping(monitor='val_accuracy', patience=3, min_delta=0.002,
                           restore_best_weights=True, verbose=1)

# âœ… åˆå§‹åŒ– MobileNetV2 å¹¶å†»ç»“å…¨éƒ¨å±‚
base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False,
                                               input_shape=(img_size, img_size, 3))
for layer in base_model.layers:
    layer.trainable = False

# âœ… ç¼–è¯‘å’Œè®­ç»ƒ
model = build_model(base_model, train_gen.num_classes)
model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=[early_stop], verbose=2)

# âœ… æµ‹è¯•é›†è¯„ä¼°
loss, acc = model.evaluate(test_gen, verbose=0)
print(f"\nâœ… Test Accuracy: {acc:.4f}")

# âœ… ä¿å­˜æ¨¡å‹
model.save("/content/drive/MyDrive/coal/mobilenetv2_freeze_model.h5")

# âœ… å¯è§†åŒ– val_accuracy
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.title(f"{ds_name} Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

"""# 2. YOLO Detection Pipelineï¼ˆYOLOv5 + YOLOv8ï¼‰
## English
This section corresponds to all Colab cells that prepare and train both YOLOv5 and YOLOv8 models.  
The purpose of this part is straightforward: **convert VOC-style annotations into YOLO format, build clean train/val/test splits, and train lightweight detectors that can reliably locate coal regions on conveyor-belt images.**

The pipeline begins with YOLOv5. After installing the repo, we convert all VOC XML labels into YOLO TXT format. Every bounding box is normalized (x_center, y_center, width, height) so the detector learns scale-invariant coordinates. Once labels are ready, we create an 80/10/10 split and copy images + labels into YOLOv5â€™s expected folder structure. A simple `coal.yaml` file defines the dataset paths and the three detection classes (â€œcoalâ€, â€œgangueâ€, â€œunknownâ€). Training is done with the standard `yolov5s.pt` checkpoint, which is small enough to train quickly while still giving solid detection performance. The resulting `best.pt` is exported for later inference.

We repeat a similar process for YOLOv8 but use a cleaner dataset structure and a single-class setting (â€œtargetâ€). The logic is identical:  
split â†’ convert XML â†’ write YOLO TXT â†’ generate data.yaml â†’ train.  
YOLOv8n is used here because it provides fast training and low latency, making it suitable for later integration into the Flask inference pipeline.

Together, these cells form a complete detection workflow: data conversion, dataset organization, YAML definition, and model training. The final weights from YOLOv5 or YOLOv8 can be plugged directly into the downstream classifier pipeline to crop ROIs for MobileNetV2.

## ä¸­æ–‡
æœ¬èŠ‚å¯¹åº”æ‰€æœ‰ YOLOv5 ä¸ YOLOv8 çš„ Colab cellã€‚  
æ•´ä½“ç›®æ ‡å¾ˆæ˜ç¡®ï¼š**æŠŠ VOC æ ‡æ³¨æ•°æ®è½¬æ¢æˆ YOLO æ ¼å¼ï¼Œæ„å»ºæ ‡å‡†åŒ–çš„ train/val/test æ•°æ®é›†ï¼Œå¹¶è®­ç»ƒè½»é‡çº§çš„ YOLO æ£€æµ‹æ¨¡å‹ï¼Œç”¨äºå®šä½ä¼ é€å¸¦ä¸Šçš„ç…¤å—åŒºåŸŸã€‚**

æµç¨‹é¦–å…ˆä» YOLOv5 å¼€å§‹ã€‚å®‰è£…ä»“åº“åï¼Œå°†æ‰€æœ‰ VOC XML æ–‡ä»¶è½¬æ¢ä¸º YOLO TXT æ ¼å¼ï¼Œæ¯ä¸ªæ¡†éƒ½è¢«å½’ä¸€åŒ–æˆ (x_center, y_center, width, height)ï¼Œè®©æ¨¡å‹å…·å¤‡å°ºåº¦ä¸å˜æ€§ã€‚æ¥ç€å»ºç«‹ 80/10/10 çš„æ•°æ®åˆ’åˆ†ï¼Œå¹¶å°†å›¾ç‰‡ä¸æ ‡ç­¾æŒ‰ YOLOv5 çš„ç›®å½•è¦æ±‚æ‹·è´ã€‚`coal.yaml` ä¸­å®šä¹‰æ•°æ®è·¯å¾„ä¸ä¸‰ä¸ªç±»åˆ«ï¼ˆâ€œcoalâ€ã€â€œgangueâ€ã€â€œunknownâ€ï¼‰ã€‚éšåä»¥ `yolov5s.pt` ä½œä¸ºåˆå§‹æƒé‡è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¨¡å‹ä½“ç§¯å°ã€è®­ç»ƒå¿«ï¼Œé€‚åˆå¿«é€ŸéªŒè¯ã€‚æœ€ç»ˆçš„ `best.pt` ä¼šç”¨äºåç»­æ¨ç†ã€‚

YOLOv8 çš„æµç¨‹ç±»ä¼¼ï¼Œä½†æ•°æ®ç»“æ„æ›´å¹²å‡€ï¼ŒåŒæ—¶é‡‡ç”¨ **å•ç±»ç›®æ ‡**ï¼ˆâ€œtargetâ€ï¼‰ã€‚æ­¥éª¤ä¸ YOLOv5 åŸºæœ¬ä¸€è‡´ï¼š  
å…ˆåˆ’åˆ†æ•°æ® â†’ å†å°† XML è½¬æˆ YOLO TXT â†’ å†™ data.yaml â†’ è®­ç»ƒ YOLOv8nã€‚  
YOLOv8nï¼ˆnanoï¼‰ç‰ˆæœ¬é€Ÿåº¦æœ€å¿«ã€å»¶è¿Ÿä½ï¼Œç‰¹åˆ«é€‚åˆä¹‹åä¸ Flask API ç»„åˆåšå®æ—¶æ¨ç†ã€‚

æ•´ä½“è€Œè¨€ï¼Œè¿™éƒ¨åˆ† Cell æ„æˆäº†å®Œæ•´çš„æ£€æµ‹ç®¡çº¿ï¼šæ ¼å¼è½¬æ¢ã€æ•°æ®ç»„ç»‡ã€yaml é…ç½®ã€æ¨¡å‹è®­ç»ƒã€‚YOLOv5 æˆ– YOLOv8 è®­ç»ƒå‡ºçš„æƒé‡éƒ½å¯ä»¥ç›´æ¥ç”¨äºä¸‹æ¸¸ MobileNetV2 åˆ†ç±»é˜¶æ®µï¼Œç”¨äºè£å‰ª ROI åŒºåŸŸè¿›è¡Œç…¤è´¨åˆ¤åˆ«ã€‚


"""

# Commented out IPython magic to ensure Python compatibility.
# âœ… STEP 1: å®‰è£… YOLOv5
!git clone https://github.com/ultralytics/yolov5
# %cd yolov5
# %pip install -r requirements.txt

# âœ… STEP 2: VOC(XML) âœ YOLO(TXT)
import os, xml.etree.ElementTree as ET, shutil, random
from pathlib import Path

# ğŸ”§ ä¿®æ”¹æˆä½ è‡ªå·±çš„è·¯å¾„ï¼ˆVOC æ ¼å¼è¾“å…¥ï¼‰
image_dir = '/content/drive/MyDrive/coal/YOLO/img'
xml_dir = '/content/drive/MyDrive/coal/YOLO/annotation'
yolo_labels_dir = '/content/drive/MyDrive/coal/YOLO/labels'
os.makedirs(yolo_labels_dir, exist_ok=True)
classes = ['coal', 'gangue', 'unknown']

def convert(size, box):
    dw, dh = 1. / size[0], 1. / size[1]
    x, y = (box[0]+box[2])/2.0, (box[1]+box[3])/2.0
    w, h = box[2]-box[0], box[3]-box[1]
    return x*dw, y*dh, w*dw, h*dh

for file in os.listdir(xml_dir):
    if not file.endswith('.xml'): continue
    tree = ET.parse(os.path.join(xml_dir, file))
    root = tree.getroot()
    size = root.find('size')
    w, h = int(size.find('width').text), int(size.find('height').text)
    with open(os.path.join(yolo_labels_dir, file.replace('.xml', '.txt')), 'w') as out_file:
        for obj in root.iter('object'):
            cls = obj.find('name').text
            if cls not in classes: continue
            cls_id = classes.index(cls)
            xmlbox = obj.find('bndbox')
            b = [int(xmlbox.find(tag).text) for tag in ['xmin','ymin','xmax','ymax']]
            bb = convert((w, h), b)
            out_file.write(f"{cls_id} {' '.join(map(str, bb))}\n")

# âœ… STEP 3: åˆ’åˆ† train/val/test æ•°æ®é›†
images = list(Path(image_dir).glob('*.jpg'))
random.shuffle(images)
split_idx = [int(len(images)*x) for x in [0.8, 0.9]]
splits = {'train': images[:split_idx[0]], 'val': images[split_idx[0]:split_idx[1]], 'test': images[split_idx[1]:]}
base_out = Path('/content/drive/MyDrive/coal/YOLO/dataset')
for split in splits:
    (base_out/split/'images').mkdir(parents=True, exist_ok=True)
    (base_out/split/'labels').mkdir(parents=True, exist_ok=True)
    for img_path in splits[split]:
        label_path = Path(yolo_labels_dir)/img_path.with_suffix('.txt').name
        if label_path.exists():
            shutil.copy(img_path, base_out/split/'images'/img_path.name)
            shutil.copy(label_path, base_out/split/'labels'/label_path.name)

# âœ… STEP 4: åˆ›å»º data.yaml é…ç½®
yaml_str = """
train: /content/drive/MyDrive/coal/YOLO/dataset/train/images
val: /content/drive/MyDrive/coal/YOLO/dataset/val/images
test: /content/drive/MyDrive/coal/YOLO/dataset/test/images

nc: 3
names: ['coal', 'gangue', 'unknown']
"""
with open("coal.yaml", "w") as f:
    f.write(yaml_str)

# âœ… STEP 5: å¼€å§‹è®­ç»ƒ YOLOv5 æ¨¡å‹
!python train.py --img 640 --batch 16 --epochs 50 --data coal.yaml --weights yolov5s.pt --cache

# âœ… STEP 6: å¤åˆ¶ best.pt åˆ°ä½ çš„ Google Drive
!cp runs/train/exp/weights/best.pt /content/drive/MyDrive/coal/models/yolov5_coal_best.pt

# Step 1: æŒ‚è½½ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: è®¾ç½®è·¯å¾„
import os
from pathlib import Path

base_dir = Path("/content/drive/MyDrive/coal/YOLO_V8")
img_dir = base_dir / "img"
anno_dir = base_dir / "annotation"
output_dir = base_dir / "yolo_dataset"

output_img = output_dir / "images"
output_anno = output_dir / "labels"

for subfolder in ["train", "val", "test"]:
    os.makedirs(output_img / subfolder, exist_ok=True)
    os.makedirs(output_anno / subfolder, exist_ok=True)

# Step 3: åˆ’åˆ†æ•°æ®é›†
from sklearn.model_selection import train_test_split
import shutil

all_imgs = sorted([f for f in os.listdir(img_dir) if f.endswith(".jpg")])
train_imgs, test_imgs = train_test_split(all_imgs, test_size=0.2, random_state=42)
train_imgs, val_imgs = train_test_split(train_imgs, test_size=0.2, random_state=42)

split_dict = {'train': train_imgs, 'val': val_imgs, 'test': test_imgs}

# Step 4: æ‹·è´å›¾ç‰‡å’Œ XML â†’ TXT è½¬æ¢
import xml.etree.ElementTree as ET

def convert_voc_to_yolo(xml_path, img_width, img_height):
    root = ET.parse(xml_path).getroot()
    yolo_lines = []

    for obj in root.findall('object'):
        bnd = obj.find("bndbox")
        xmin = int(bnd.find("xmin").text)
        ymin = int(bnd.find("ymin").text)
        xmax = int(bnd.find("xmax").text)
        ymax = int(bnd.find("ymax").text)

        # è½¬ä¸ºç›¸å¯¹åæ ‡
        x_center = (xmin + xmax) / 2 / img_width
        y_center = (ymin + ymax) / 2 / img_height
        width = (xmax - xmin) / img_width
        height = (ymax - ymin) / img_height

        # ç±»åˆ«å›ºå®šä¸º 0ï¼ˆä»»æ„ç›®æ ‡ï¼‰
        yolo_lines.append(f"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}")

    return yolo_lines

from PIL import Image

for split, img_list in split_dict.items():
    for img_name in img_list:
        # æ‹·è´å›¾åƒ
        src_img_path = img_dir / img_name
        dst_img_path = output_img / split / img_name
        shutil.copy(src_img_path, dst_img_path)

        # è½¬æ¢æ ‡æ³¨
        xml_path = anno_dir / img_name.replace(".jpg", ".xml")
        dst_txt_path = output_anno / split / img_name.replace(".jpg", ".txt")

        if not xml_path.exists():
            print(f"Warning: {xml_path} not found!")
            continue

        with Image.open(src_img_path) as img:
            w, h = img.size

        yolo_annos = convert_voc_to_yolo(xml_path, w, h)
        with open(dst_txt_path, "w") as f:
            f.write("\n".join(yolo_annos))

# Step 5: åˆ›å»º data.yaml
data_yaml = f"""
path: {output_dir}
train: images/train
val: images/val
test: images/test

names:
  0: target
"""

with open(base_dir / "data.yaml", "w") as f:
    f.write(data_yaml.strip())

# Step 6: å®‰è£… ultralytics å¹¶è®­ç»ƒ YOLOv8
!pip install -q ultralytics
from ultralytics import YOLO

model = YOLO("yolov8n.yaml")  # ä½¿ç”¨ nano ç‰ˆæœ¬ï¼ˆæœ€å¿«ï¼‰
model.train(
    data=str(base_dir / "data.yaml"),
    epochs=20,
    imgsz=640,
    batch=16,
    project=str(base_dir / "runs"),
    name="exp",
    exist_ok=True
)

"""# 3. End-to-End Inference Service (Flask + YOLO + MobileNetV2)
## English
This part describes how we wrap the whole pipeline into a single, stateless HTTP service using Flask, with YOLO as the detector and MobileNetV2 as the quality classifier.

At startup, the Flask app loads all heavy assets once into memory:
- YOLOv5/YOLOv8 weights for **object detection** (coal region localization)  
- The frozen **MobileNetV2** classifier (`mobilenetv2_freeze_model.h5`) for coal quality prediction  

We then expose a `POST /predict` endpoint. A typical request sends one conveyor-belt image (e.g., multipart/form-data or base64). The handler does three things in sequence:

1. **Detection**  
   Pass the image through YOLO, filter out low-confidence boxes, and get bounding boxes for all coal-like regions on the belt.

2. **Cropping + Preprocessing**  
   For each box, crop the ROI, resize to 224Ã—224, apply the same normalization as in training (rescale, optional CLAHE), and stack them into a small batch.

3. **Classification & Aggregation**  
   Run the ROI batch through MobileNetV2 to obtain class probabilities for each patch. Then aggregate these patch-level predictions into interpretable metrics, such as:
   - per-patch quality labels (high / acceptable / poor)  
   - overall quality distribution for the whole image or batch  
   - optional confidence scores

The response is a JSON object, e.g.:
- list of bounding boxes + class labels  
- overall coal quality summary for the truck / belt segment  

To ensure latency stays under ~1s, the app:
- loads models only once at startup (not per request)  
- limits input resolution and max number of ROIs per request  
- runs on a small EC2 instance or edge server, optionally behind gunicorn with multiple workers

From the outside, it is just a REST API: send an image, get structured quality results back. Internally, it is the full YOLO + MobileNetV2 pipeline we built, wrapped into a clean, production-style service.

## ä¸­æ–‡
è¿™ä¸€éƒ¨åˆ†ä»‹ç»å¦‚ä½•ç”¨ Flask æŠŠæ•´ä¸ªæµç¨‹å°è£…æˆä¸€ä¸ª **ç«¯åˆ°ç«¯åœ¨çº¿æ¨ç†æœåŠ¡**ï¼Œå‰åŠæ®µç”¨ YOLO åšæ£€æµ‹ï¼ŒååŠæ®µç”¨ MobileNetV2 åšç…¤è´¨åˆ†ç±»ã€‚

æœåŠ¡å¯åŠ¨æ—¶ï¼ŒFlask è¿›ç¨‹ä¼šä¸€æ¬¡æ€§æŠŠæ‰€æœ‰å¤§æ¨¡å‹åŠ è½½åˆ°å†…å­˜ä¸­ï¼š
- YOLOv5 / YOLOv8 æƒé‡ï¼ˆè´Ÿè´£è¯†åˆ«ä¼ é€å¸¦ä¸Šçš„ç…¤å—ä½ç½®ï¼‰  
- è®­ç»ƒå¥½çš„ **MobileNetV2 åˆ†ç±»æ¨¡å‹**ï¼ˆ`mobilenetv2_freeze_model.h5`ï¼Œè´Ÿè´£åˆ¤æ–­ç…¤è´¨ç­‰çº§ï¼‰  

ç„¶åæš´éœ²ä¸€ä¸ª `POST /predict` æ¥å£ã€‚å®¢æˆ·ç«¯ä¸Šä¼ ä¸€å¼ ä¼ é€å¸¦å›¾åƒï¼ˆæ¯”å¦‚è¡¨å•å›¾ç‰‡æˆ– base64ï¼‰ï¼Œåç«¯æŒ‰é¡ºåºæ‰§è¡Œä¸‰æ­¥ï¼š

1. **ç›®æ ‡æ£€æµ‹**  
   ç”¨ YOLO å¯¹æ•´å¼ å›¾åšé¢„æµ‹ï¼Œè¿‡æ»¤æ‰ä½ç½®ä¿¡åº¦æ¡†ï¼Œå¾—åˆ°å½“å‰ç”»é¢ä¸­çš„æ‰€æœ‰ç…¤å—åŒºåŸŸã€‚

2. **è£å‰ª + é¢„å¤„ç†**  
   å¯¹æ¯ä¸ªæ£€æµ‹æ¡†ï¼Œä»åŸå›¾è£å‰ª ROIï¼Œç»Ÿä¸€ resize åˆ° 224Ã—224ï¼Œå¹¶åšä¸è®­ç»ƒé˜¶æ®µä¸€è‡´çš„é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ–ã€å¯é€‰ CLAHEï¼‰ï¼Œæ‰“åŒ…æˆä¸€ä¸ªå° batchã€‚

3. **åˆ†ç±»ä¸æ±‡æ€»**  
   å°† ROI batch è¾“å…¥ MobileNetV2ï¼Œå¾—åˆ°æ¯ä¸ªå°å—çš„ç…¤è´¨ç±»åˆ«æ¦‚ç‡ã€‚  
   å†å°†è¿™äº› patch çº§åˆ«çš„ç»“æœèšåˆæˆæ›´æ˜“ç”¨çš„æŒ‡æ ‡ï¼Œæ¯”å¦‚ï¼š
   - æ¯ä¸ªæ¡†å¯¹åº”çš„ç…¤è´¨ç­‰çº§ï¼ˆé«˜ / å¯æ¥å— / å·®ï¼‰  
   - æ•´è½¦æˆ–æ•´æ®µçš®å¸¦çš„å¤§è‡´åˆæ ¼ç‡ / è´¨é‡åˆ†å¸ƒ  
   - å¯¹åº”çš„ç½®ä¿¡åº¦ä¿¡æ¯  

æ¥å£è¿”å›çš„æ˜¯ä¸€ä¸ª JSONï¼Œä¾‹å¦‚ï¼š
- æ¯ä¸ª bounding box çš„ä½ç½® + åˆ†ç±»æ ‡ç­¾  
- å½“å‰è¿™è½¦ç…¤æ•´ä½“è´¨é‡çš„æ±‡æ€»ç»“æœ  

ä¸ºäº†æŠŠå»¶è¿Ÿæ§åˆ¶åœ¨çº¦ 1 ç§’å†…ï¼ŒæœåŠ¡ä¼šï¼š
- åªåœ¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ï¼Œè€Œä¸æ˜¯æ¯æ¬¡è¯·æ±‚éƒ½é‡æ–°åŠ è½½  
- é™åˆ¶è¾“å…¥å›¾ç‰‡åˆ†è¾¨ç‡å’Œæ¯æ¬¡è¯·æ±‚çš„ ROI æ•°é‡  
- åœ¨å°è§„æ ¼ EC2 æˆ–è¾¹ç¼˜æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œé€šå¸¸é…åˆ gunicorn å¤š worker æå‡å¹¶å‘èƒ½åŠ›  

å¯¹å¤–æ¥çœ‹ï¼Œè¿™åªæ˜¯ä¸€ä¸ªç®€å•çš„ REST APIï¼š  
â€œä¸Šä¼ å›¾ç‰‡ â†’ è¿”å›ç»“æ„åŒ–çš„ç…¤è´¨æ£€æµ‹ç»“æœâ€ï¼›  
å¯¹å†…ï¼Œå®ƒå°è£…çš„æ˜¯æˆ‘ä»¬å®Œæ•´çš„ YOLO + MobileNetV2 æ¨ç†é“¾è·¯ï¼Œå¹¶ä»¥å·¥ç¨‹å¯è½åœ°çš„æ–¹å¼å¯¹å¤–æä¾›æœåŠ¡ã€‚


"""

"""
app.py  â€“  End-to-End Inference Service (Flask + YOLO + MobileNetV2)

English:
    - Loads YOLO (detection) and MobileNetV2 (classification) once at startup.
    - Exposes two endpoints:
        * GET /health      â€“ quick health check
        * POST /predict    â€“ accepts an image, returns JSON with boxes + quality labels

ä¸­æ–‡ï¼š
    - æœåŠ¡å¯åŠ¨æ—¶ä¸€æ¬¡æ€§åŠ è½½ YOLOï¼ˆæ£€æµ‹ï¼‰å’Œ MobileNetV2ï¼ˆåˆ†ç±»ï¼‰æ¨¡å‹
    - æä¾›ä¸¤ä¸ªæ¥å£ï¼š
        * GET /health      â€“ å¥åº·æ£€æŸ¥
        * POST /predict    â€“ ä¸Šä¼ å›¾ç‰‡ï¼Œè¿”å›æ¡†ä½ç½® + ç…¤è´¨åˆ†ç±»ç»“æœï¼ˆJSONï¼‰
"""

import io
import os
import time
import numpy as np
from typing import List, Dict

from flask import Flask, request, jsonify
from PIL import Image

import tensorflow as tf
from tensorflow.keras.models import load_model

from ultralytics import YOLO

# -----------------------------
# Config
# -----------------------------
YOLO_WEIGHTS_PATH = "/content/drive/MyDrive/coal/YOLO_V8/train_run/weights/best.pt"
CLASSIFIER_WEIGHTS_PATH = "/content/drive/MyDrive/coal/mobilenetv2_freeze_model.h5"

IMG_SIZE = 224
MIN_CONF = 0.3          # minimum YOLO confidence to keep a box
MAX_ROIS = 32           # max number of cropped patches per request

QUALITY_LABELS = ["high", "acceptable", "poor", "background"]  # index 0..3


# -----------------------------
# Model Loading (run once)
# -----------------------------
app = Flask(__name__)

print("[INFO] Loading YOLO model...")
yolo_model = YOLO(YOLO_WEIGHTS_PATH)

print("[INFO] Loading MobileNetV2 classifier...")
classifier_model = load_model(CLASSIFIER_WEIGHTS_PATH)

print("[INFO] Models loaded. Ready to serve requests.")


# -----------------------------
# Utility Functions
# -----------------------------
def read_image_from_request(req) -> Image.Image:
    """
    Read image from Flask request (form-data: 'image') and return a PIL Image.
    è¯·æ±‚ä¸­è¯»å–å›¾ç‰‡ï¼ˆè¡¨å•å­—æ®µåä¸º 'image'ï¼‰ï¼Œè¿”å› PIL Imageã€‚
    """
    if 'image' not in req.files:
        raise ValueError("No file part named 'image' in the request.")
    file = req.files['image']
    if file.filename == '':
        raise ValueError("Empty filename.")
    img_bytes = file.read()
    return Image.open(io.BytesIO(img_bytes)).convert("RGB")


def run_yolo_detection(pil_img: Image.Image):
    """
    Run YOLO detection on a PIL image and return boxes, confs, classes.
    åœ¨ PIL å›¾ä¸Šè·‘ YOLOï¼Œè¿”å›æ¡†åæ ‡ã€ç½®ä¿¡åº¦å’Œç±»åˆ«ç´¢å¼•ã€‚
    """
    results = yolo_model(pil_img, verbose=False)
    if len(results) == 0:
        return [], [], []

    res = results[0]
    if res.boxes is None or res.boxes.xyxy is None:
        return [], [], []

    xyxy = res.boxes.xyxy.cpu().numpy()
    conf = res.boxes.conf.cpu().numpy()
    cls = res.boxes.cls.cpu().numpy().astype(int)
    return xyxy, conf, cls


def crop_and_preprocess_rois(pil_img: Image.Image, boxes: np.ndarray) -> np.ndarray:
    """
    Crop ROIs from PIL image using YOLO boxes and preprocess for MobileNetV2.
    æ ¹æ® YOLO æ£€æµ‹æ¡†è£å‰ª ROIï¼Œå¹¶é¢„å¤„ç†æˆ MobileNetV2 è¾“å…¥æ ¼å¼ã€‚
    """
    rois = []
    w, h = pil_img.size

    for i, (x1, y1, x2, y2) in enumerate(boxes):
        if i >= MAX_ROIS:
            break
        # clamp to image boundary
        x1_clamp = max(0, int(x1))
        y1_clamp = max(0, int(y1))
        x2_clamp = min(w, int(x2))
        y2_clamp = min(h, int(y2))
        if x2_clamp <= x1_clamp or y2_clamp <= y1_clamp:
            continue

        roi = pil_img.crop((x1_clamp, y1_clamp, x2_clamp, y2_clamp))
        roi = roi.resize((IMG_SIZE, IMG_SIZE))
        roi_arr = np.array(roi).astype("float32") / 255.0  # same normalization as training
        rois.append(roi_arr)

    if not rois:
        return np.zeros((0, IMG_SIZE, IMG_SIZE, 3), dtype="float32")
    return np.stack(rois, axis=0)


def classify_rois(roi_batch: np.ndarray) -> np.ndarray:
    """
    Run MobileNetV2 classifier on ROI batch, return softmax probabilities.
    å¯¹ ROI batch è¿›è¡Œåˆ†ç±»ï¼Œè¿”å› softmax æ¦‚ç‡çŸ©é˜µã€‚
    """
    if roi_batch.shape[0] == 0:
        return np.zeros((0, len(QUALITY_LABELS)), dtype="float32")
    probs = classifier_model.predict(roi_batch, verbose=0)
    return probs


def aggregate_quality(probs: np.ndarray) -> Dict:
    """
    Aggregate patch-level predictions into a summary (count per label).
    å°†æ¯ä¸ª patch çš„åˆ†ç±»ç»“æœèšåˆæˆæ•´ä½“è´¨é‡ç»Ÿè®¡ã€‚
    """
    if probs.shape[0] == 0:
        return {"total_patches": 0, "distribution": {}, "majority_label": None}

    pred_idx = np.argmax(probs, axis=1)
    counts = {label: 0 for label in QUALITY_LABELS}
    for idx in pred_idx:
        label = QUALITY_LABELS[idx]
        counts[label] += 1

    total = int(probs.shape[0])
    # compute ratio
    dist = {
        label: {
            "count": int(cnt),
            "ratio": float(cnt) / total if total > 0 else 0.0
        }
        for label, cnt in counts.items()
    }
    # majority label
    majority_label = max(counts.items(), key=lambda x: x[1])[0]

    return {
        "total_patches": total,
        "distribution": dist,
        "majority_label": majority_label
    }


# -----------------------------
# Flask Endpoints
# -----------------------------
@app.route("/health", methods=["GET"])
def health():
    """
    Simple health check endpoint.
    å¥åº·æ£€æŸ¥æ¥å£ã€‚
    """
    return jsonify({"status": "ok"}), 200


@app.route("/predict", methods=["POST"])
def predict():
    """
    Main inference endpoint.
    - Input: image file (form-data, key='image')
    - Output: JSON with YOLO boxes + per-ROI quality + aggregated quality summary

    ä¸»æ¨ç†æ¥å£ï¼š
    - è¾“å…¥ï¼šè¡¨å•ä¸Šä¼ çš„å›¾ç‰‡ï¼ˆå­—æ®µå 'image'ï¼‰
    - è¾“å‡ºï¼šåŒ…å«æ£€æµ‹æ¡†ã€æ¯ä¸ª ROI ç…¤è´¨ç­‰çº§ä»¥åŠæ•´ä½“ç»Ÿè®¡çš„ JSONã€‚
    """
    start_time = time.time()
    try:
        pil_img = read_image_from_request(request)
    except Exception as e:
        return jsonify({"error": str(e)}), 400

    # 1. YOLO detection
    boxes, confs, yolo_classes = run_yolo_detection(pil_img)
    if not boxes:
        elapsed = time.time() - start_time
        return jsonify({
            "num_detections": 0,
            "boxes": [],
            "summary": {
                "total_patches": 0,
                "distribution": {},
                "majority_label": None
            },
            "latency_sec": elapsed
        }), 200

    # filter by confidence
    boxes = np.array(boxes)
    confs = np.array(confs)
    yolo_classes = np.array(yolo_classes)

    keep_mask = confs >= MIN_CONF
    boxes = boxes[keep_mask]
    confs = confs[keep_mask]
    yolo_classes = yolo_classes[keep_mask]

    if boxes.shape[0] == 0:
        elapsed = time.time() - start_time
        return jsonify({
            "num_detections": 0,
            "boxes": [],
            "summary": {
                "total_patches": 0,
                "distribution": {},
                "majority_label": None
            },
            "latency_sec": elapsed
        }), 200

    # 2. Crop + preprocess ROIs
    roi_batch = crop_and_preprocess_rois(pil_img, boxes)

    # 3. Classification
    probs = classify_rois(roi_batch)
    summary = aggregate_quality(probs)

    # per-ROI details
    roi_results: List[Dict] = []
    for i in range(probs.shape[0]):
        q_idx = int(np.argmax(probs[i]))
        q_label = QUALITY_LABELS[q_idx]
        q_conf = float(np.max(probs[i]))
        x1, y1, x2, y2 = boxes[i].tolist()
        roi_results.append({
            "bbox": [float(x1), float(y1), float(x2), float(y2)],
            "yolo_cls": int(yolo_classes[i]),
            "yolo_conf": float(confs[i]),
            "quality_label": q_label,
            "quality_prob": q_conf
        })

    elapsed = time.time() - start_time
    resp = {
        "num_detections": len(roi_results),
        "boxes": roi_results,
        "summary": summary,
        "latency_sec": elapsed
    }
    return jsonify(resp), 200


if __name__ == "__main__":
    # For local testing / debug. In production, run with gunicorn:
    # gunicorn -w 2 -b 0.0.0.0:8000 app:app
    app.run(host="0.0.0.0", port=8000, debug=False)